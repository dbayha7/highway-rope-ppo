{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03dc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install numpy\n",
    "#!pip install joblib\n",
    "#!pip install pathlib\n",
    "#!pip install highway_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e643f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import highway_env\n",
    "from gymnasium.spaces import Box\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7bf6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, flatten_obs=True):\n",
    "        super(HighwayEnvWrapper, self).__init__(env)\n",
    "        self.flatten_obs = flatten_obs\n",
    "\n",
    "        # Store original shape before flattening\n",
    "        original_shape = env.observation_space.shape\n",
    "\n",
    "        if flatten_obs:\n",
    "            flat_shape = int(np.prod(original_shape))\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=-1.0,\n",
    "                high=1.0,\n",
    "                shape=(flat_shape,),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = env.observation_space\n",
    "\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        if self.flatten_obs:\n",
    "            obs = obs.astype(np.float32).reshape(-1)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if self.flatten_obs:\n",
    "            obs = obs.astype(np.float32).reshape(-1)\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the top with other constants\n",
    "ARTIFACTS_DIR = os.path.join(\"artifacts\", \"highway-ppo\")\n",
    "\n",
    "\n",
    "# Then ensure_artifacts_dir\n",
    "def ensure_artifacts_dir(custom_path=None):\n",
    "    \"\"\"Create the artifacts directory if it doesn't exist.\"\"\"\n",
    "    artifacts_dir = custom_path or ARTIFACTS_DIR\n",
    "    os.makedirs(artifacts_dir, exist_ok=True)\n",
    "    return artifacts_dir\n",
    "\n",
    "\n",
    "# Directory to store log files (under artifacts)\n",
    "LOGS_DIR = os.path.join(ARTIFACTS_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0aa947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging functionality\n",
    "def setup_master_logger(log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Create and configure the master logger.\n",
    "    This logger writes to:\n",
    "      - A single 'master.log' file in LOGS_DIR\n",
    "      - The console (stdout) with INFO-level\n",
    "    \"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "    # Create a unique log file name with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    master_log_path = os.path.join(LOGS_DIR, f\"{timestamp}_master.log\")\n",
    "\n",
    "    # Configure master logger\n",
    "    logger = logging.getLogger(\"master_logger\")\n",
    "    logger.setLevel(log_level)\n",
    "    logger.handlers = []  # Clear any existing handlers\n",
    "\n",
    "    # File handler for master.log\n",
    "    fh = logging.FileHandler(master_log_path)\n",
    "    fh.setLevel(log_level)\n",
    "    fh.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Console handler at INFO-level\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(log_level)\n",
    "    ch.setFormatter(\n",
    "        logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\", \"%H:%M:%S\")\n",
    "    )\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    logger.info(f\"Master logger initialized. Log file: {master_log_path}\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b4bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment_logger(\n",
    "    experiment_id, log_level=logging.INFO, console_level=logging.WARNING\n",
    "):\n",
    "    \"\"\"\n",
    "    Create and configure a per-experiment logger.\n",
    "    Writes detailed logs to 'experiment_<id>.log'\n",
    "    Logs only warnings or higher to the console to avoid spamming.\n",
    "    \"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "    # Create a unique log file name with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    logger_name = f\"experiment_{experiment_id}\"\n",
    "    exp_log_path = os.path.join(LOGS_DIR, f\"{timestamp}_{logger_name}.log\")\n",
    "\n",
    "    # Configure experiment logger\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(log_level)\n",
    "    logger.handlers = []  # Clear any existing handlers\n",
    "\n",
    "    # File handler for experiment logs - capture all logs\n",
    "    fh = logging.FileHandler(exp_log_path)\n",
    "    fh.setLevel(log_level)\n",
    "    fh.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Console handler - only show important messages\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(console_level)  # Higher level to reduce console noise\n",
    "    ch.setFormatter(\n",
    "        logging.Formatter(\n",
    "            \"%(asctime)s | %(name)s | %(levelname)s | %(message)s\", \"%H:%M:%S\"\n",
    "        )\n",
    "    )\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Experiment logger initialized for experiment_{experiment_id}. Log file: {exp_log_path}\"\n",
    "    )\n",
    "    return logger\n",
    "\n",
    "# For backward compatibility with existing code\n",
    "def setup_logger(experiment_name=\"\", log_level=logging.INFO):\n",
    "    \"\"\"Legacy function for backward compatibility\"\"\"\n",
    "    if experiment_name:\n",
    "        return setup_experiment_logger(experiment_name, log_level)\n",
    "    else:\n",
    "        return setup_master_logger(log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18387b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if highway_env is not registered, register it\n",
    "if \"highway-v0\" not in gym.envs.registry:\n",
    "    highway_env._register_highway_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc22756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation uses highway-env's built-in observation normalization\n",
    "# By setting \"normalize\": True in the environment config, observations are\n",
    "# automatically scaled to the range [-1, 1] based on features_range\n",
    "\n",
    "# Constants for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# PPO Hyperparameters\n",
    "LEARNING_RATE = 1e-4  # Learning rate for optimizer (typical: 1e-4 to 3e-4)\n",
    "GAMMA = 0.99  # Discount factor (typical: 0.99, sometimes 0.999)\n",
    "LAMBDA = 0.95  # GAE parameter (typical: 0.95-0.98)\n",
    "EPSILON_CLIP = 0.2  # PPO clip range (typical: 0.2 - default in PPO paper)\n",
    "ENTROPY_COEF = 0.005  # Entropy coefficient (typical: 0.001-0.01)\n",
    "VALUE_COEF = 0.5  # Value function coefficient (typical: 0.5)\n",
    "MAX_GRAD_NORM = 0.5  # Maximum gradient norm for clipping\n",
    "EPOCHS = 6  # Number of epochs when optimizing (typical: 4-10 epochs per batch)\n",
    "BATCH_SIZE = 64  # Mini-batch size (typical: 64)\n",
    "HIDDEN_DIM = 128  # Size of hidden layers in network (typical: 64-256)\n",
    "STEPS_PER_UPDATE = (\n",
    "    2048  # Number of steps to collect before updating policy (typical: 1024-2048)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3e2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "MAX_EPISODES = 1500  # Maximum number of episodes to train\n",
    "TARGET_REWARD = (\n",
    "    20.0  # Target reward to consider environment solved (adjusted for highway-env)\n",
    ")\n",
    "LOG_INTERVAL = 10  # How often to log training progress\n",
    "EVAL_INTERVAL = 50  # How often to run evaluation\n",
    "\n",
    "# Highway environment configuration\n",
    "HIGHWAY_CONFIG = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,  # Number of vehicles to observe\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\"],  # Features to include\n",
    "        \"normalize\": True,  # Use built-in highway-env normalization\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-30, 30],\n",
    "            \"vy\": [-30, 30],\n",
    "            \"presence\": [0, 1],\n",
    "            \"cos_h\": [-1, 1],\n",
    "            \"sin_h\": [-1, 1],\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"random\",\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",  # Continuous steering + throttle\n",
    "        \"longitudinal\": True,  # Enable acceleration control\n",
    "        \"lateral\": True,  # Enable steering control\n",
    "    },\n",
    "    \"simulation_frequency\": 15,  # Simulation steps per second\n",
    "    \"policy_frequency\": 5,  # Decision frequency\n",
    "    \"duration\": 40,  # Episode duration in seconds\n",
    "    \"lanes_count\": 3,  # Number of lanes\n",
    "    \"vehicles_count\": 50,  # Total vehicles in the environment\n",
    "    \"vehicles_density\": 2,  # Initial density of vehicles\n",
    "    \"collision_reward\": -1,  # Reward for colliding with a vehicle\n",
    "    \"right_lane_reward\": 0.1,  # Reward for driving on the right lane\n",
    "    \"high_speed_reward\": 0.4,  # Reward for driving at full speed\n",
    "    \"lane_change_reward\": -0.05,  # Reward for changing lanes\n",
    "    \"reward_speed_range\": [20, 30],  # Speed range for positive reward\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f267a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for training.\n"
     ]
    }
   ],
   "source": [
    "# Setup reproducibility across all libraries\n",
    "def set_random_seeds(seed=SEED, exact_reproducibility=False):\n",
    "    \"\"\"Set random seeds for reproducibility across all relevant libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Only enforce deterministic behavior if exact reproducibility needed\n",
    "    if exact_reproducibility:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        # Allow cuDNN to benchmark and select fastest algorithms\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# Determine the optimal compute device\n",
    "def get_device():\n",
    "    \"\"\"Determine and return the best available compute device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), f\"GPU: {torch.cuda.get_device_name(0)}\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\"), \"Apple Silicon GPU\"\n",
    "    else:\n",
    "        return torch.device(\"cpu\"), \"CPU\"\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_random_seeds()\n",
    "\n",
    "# Set device for training\n",
    "device, device_name = get_device()\n",
    "print(f\"Using {device_name} for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335f8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # Shared feature extractor with ReLU activations\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Actor mean (for continuous actions)\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            # Remove Tanh here as we'll apply it after sampling\n",
    "        )\n",
    "\n",
    "        # Log standard deviation (learnable parameter)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "        # Critic (Value Function) head\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert numpy arrays to tensors\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "\n",
    "        shared_features = self.shared(x)\n",
    "\n",
    "        # Actor: action mean and std\n",
    "        action_mean = self.actor_mean(shared_features)\n",
    "        action_std = self.log_std.exp()\n",
    "\n",
    "        # Critic: state value\n",
    "        state_value = self.critic(shared_features)\n",
    "\n",
    "        return action_mean, action_std, state_value\n",
    "\n",
    "    def get_action(self, state, deterministic=False):\n",
    "        # Get mean and std\n",
    "        action_mean, action_std, value = self.forward(state)\n",
    "\n",
    "        # Create normal distribution\n",
    "        normal_dist = Normal(action_mean, action_std)\n",
    "\n",
    "        if deterministic:\n",
    "            # For deterministic, just use the mean (no sampling)\n",
    "            z = action_mean\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = None  # Not needed for deterministic actions\n",
    "        else:\n",
    "            # Sample from normal distribution (pre-tanh)\n",
    "            z = normal_dist.sample()\n",
    "            # Apply tanh to bound actions to [-1, 1]\n",
    "            action = torch.tanh(z)\n",
    "\n",
    "            # Compute log_prob with change of variables formula\n",
    "            # log π(a) = log π(z) - log(1 - tanh²(z))\n",
    "            # = log π(z) - sum(log(1 - a²))\n",
    "            log_prob = normal_dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "            log_prob = log_prob.sum(dim=-1)\n",
    "\n",
    "        return (\n",
    "            action.cpu().numpy(),\n",
    "            z.cpu().numpy(),\n",
    "            log_prob.item() if log_prob is not None else None,\n",
    "            value.cpu().numpy()[0],\n",
    "        )\n",
    "\n",
    "    def evaluate(self, states, actions, pre_tanh_actions):\n",
    "        # Get action distribution parameters and state values\n",
    "        action_means, action_stds, state_values = self.forward(states)\n",
    "\n",
    "        # Create normal distributions\n",
    "        dist = Normal(action_means, action_stds)\n",
    "\n",
    "        # Get log probabilities using pre-tanh actions\n",
    "        log_probs = dist.log_prob(pre_tanh_actions)\n",
    "\n",
    "        # Apply change of variables formula for tanh transformation\n",
    "        tanh_actions = torch.tanh(pre_tanh_actions)\n",
    "        log_probs = log_probs - torch.log(1 - tanh_actions.pow(2) + 1e-6)\n",
    "        log_probs = log_probs.sum(dim=-1)\n",
    "\n",
    "        # Get entropy (sum across action dimensions)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "        return log_probs, state_values, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d34038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Buffer\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size=64):\n",
    "        self.states = []\n",
    "        self.actions = []  # Store post-Tanh actions for environment interaction\n",
    "        self.pre_tanh_actions = []  # Store pre-Tanh actions for correct log_prob calculation\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def store(\n",
    "        self, state, action, pre_tanh_action, reward, next_state, log_prob, done, value\n",
    "    ):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.pre_tanh_actions.append(pre_tanh_action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.pre_tanh_actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def compute_advantages(self, gamma=0.99, lam=0.95, last_value=0):\n",
    "        rewards = np.array(self.rewards)\n",
    "        dones = np.array(self.dones)\n",
    "        values = np.array(self.values + [last_value])\n",
    "\n",
    "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "        last_advantage = 0\n",
    "\n",
    "        # Calculate GAE (Generalized Advantage Estimation)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # If it's the terminal state, there's no next value, so we set the delta to reward - value\n",
    "            # Otherwise, we calculate delta as reward + gamma * next_value * (1 - done) - value\n",
    "            delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage\n",
    "            last_advantage = advantages[t]\n",
    "\n",
    "        # Calculate returns\n",
    "        returns = advantages + np.array(self.values)\n",
    "\n",
    "        return advantages, returns\n",
    "\n",
    "    def get_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start_indices = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i : i + self.batch_size] for i in batch_start_indices]\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def get_tensors(self):\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        actions = torch.FloatTensor(np.array(self.actions)).to(device)\n",
    "        pre_tanh_actions = torch.FloatTensor(np.array(self.pre_tanh_actions)).to(device)\n",
    "        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).to(device)\n",
    "\n",
    "        return states, actions, pre_tanh_actions, old_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1dcd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        lam=LAMBDA,\n",
    "        eps_clip=EPSILON_CLIP,\n",
    "        value_coef=VALUE_COEF,\n",
    "        entropy_coef=ENTROPY_COEF,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        logger=None,\n",
    "    ):\n",
    "        self.actor_critic = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.epochs = epochs\n",
    "        self.logger = logger or logging.getLogger(\"ppo_highway\")\n",
    "\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            # Get action, pre_tanh_action, log probability, and value\n",
    "            action, pre_tanh_action, log_prob, value = self.actor_critic.get_action(\n",
    "                state, deterministic\n",
    "            )\n",
    "\n",
    "        return action, pre_tanh_action, log_prob, value\n",
    "\n",
    "    def update(self, last_value=0.0):\n",
    "        # Get data from memory\n",
    "        states, actions, pre_tanh_actions, old_log_probs = self.memory.get_tensors()\n",
    "        advantages, returns = self.memory.compute_advantages(\n",
    "            self.gamma, self.lam, last_value\n",
    "        )\n",
    "\n",
    "        # Convert to tensors\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "        # Normalize advantages (helpful for stable training)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Get batches\n",
    "        batches = self.memory.get_batches()\n",
    "\n",
    "        # Metrics to track\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        total_loss = 0\n",
    "        clip_fraction = 0\n",
    "        approx_kl_div = 0\n",
    "        explained_var = 0\n",
    "\n",
    "        # Optimization loop\n",
    "        for epoch in trange(self.epochs, desc=\"PPO Training Epochs\"):\n",
    "            epoch_policy_loss = 0\n",
    "            epoch_value_loss = 0\n",
    "            epoch_entropy = 0\n",
    "            epoch_total_loss = 0\n",
    "            epoch_clip_count = 0\n",
    "            epoch_kl_sum = 0\n",
    "\n",
    "            for batch_indices in batches:\n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_pre_tanh_actions = pre_tanh_actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "\n",
    "                # Evaluate actions using pre-tanh actions for correct log probs\n",
    "                new_log_probs, state_values, entropy = self.actor_critic.evaluate(\n",
    "                    batch_states, batch_actions, batch_pre_tanh_actions\n",
    "                )\n",
    "\n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "\n",
    "                # Calculate KL divergence\n",
    "                with torch.no_grad():\n",
    "                    log_ratio = new_log_probs - batch_old_log_probs\n",
    "                    batch_kl = ((torch.exp(log_ratio) - 1) - log_ratio).mean().item()\n",
    "                    epoch_kl_sum += batch_kl\n",
    "\n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = (\n",
    "                    torch.clamp(ratios, 1.0 - self.eps_clip, 1.0 + self.eps_clip)\n",
    "                    * batch_advantages\n",
    "                )\n",
    "\n",
    "                # Calculate actor loss (negative because we perform gradient ascent)\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Calculate critic loss - ensure shapes are compatible\n",
    "                state_values = state_values.squeeze(-1)  # Fix potential shape issues\n",
    "                critic_loss = F.mse_loss(state_values, batch_returns)\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy_bonus = entropy.mean()\n",
    "                loss = (\n",
    "                    actor_loss\n",
    "                    + self.value_coef * critic_loss\n",
    "                    - self.entropy_coef * entropy_bonus\n",
    "                )\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    self.actor_critic.parameters(), self.max_grad_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Count clipped samples\n",
    "                with torch.no_grad():\n",
    "                    clip_count = (\n",
    "                        (torch.abs(ratios - 1.0) > self.eps_clip).float().sum().item()\n",
    "                    )\n",
    "                    epoch_clip_count += clip_count / len(batch_indices)\n",
    "\n",
    "                # Accumulate losses for this epoch\n",
    "                epoch_policy_loss += actor_loss.item()\n",
    "                epoch_value_loss += critic_loss.item()\n",
    "                epoch_entropy += entropy_bonus.item()\n",
    "                epoch_total_loss += loss.item()\n",
    "\n",
    "            # Average over batches\n",
    "            num_batches = len(batches)\n",
    "            epoch_policy_loss /= num_batches\n",
    "            epoch_value_loss /= num_batches\n",
    "            epoch_entropy /= num_batches\n",
    "            epoch_total_loss /= num_batches\n",
    "            epoch_clip_fraction = epoch_clip_count / num_batches\n",
    "            epoch_approx_kl = epoch_kl_sum / num_batches\n",
    "\n",
    "            # Accumulate for overall metrics\n",
    "            total_policy_loss += epoch_policy_loss\n",
    "            total_value_loss += epoch_value_loss\n",
    "            total_entropy += epoch_entropy\n",
    "            total_loss += epoch_total_loss\n",
    "            clip_fraction += epoch_clip_fraction\n",
    "            approx_kl_div += epoch_approx_kl\n",
    "\n",
    "            # Log epoch details in compact key=value format at DEBUG level\n",
    "            self.logger.debug(\n",
    "                \"epoch=%d/%d loss=%.4f policy_loss=%.4f value_loss=%.4f entropy=%.4f clip_frac=%.3f kl=%.5f\",\n",
    "                epoch + 1,\n",
    "                self.epochs,\n",
    "                epoch_total_loss,\n",
    "                epoch_policy_loss,\n",
    "                epoch_value_loss,\n",
    "                epoch_entropy,\n",
    "                epoch_clip_fraction,\n",
    "                epoch_approx_kl,\n",
    "            )\n",
    "\n",
    "        # Calculate explained variance\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.FloatTensor(self.memory.values).to(device)\n",
    "            y_true = returns[:-1] if len(returns) > len(y_pred) else returns\n",
    "            var_y = torch.var(y_true)\n",
    "            explained_var = 1 - torch.var(y_true - y_pred) / var_y if var_y > 0 else 0\n",
    "            explained_var = explained_var.item()\n",
    "\n",
    "        # Average over epochs\n",
    "        avg_policy_loss = total_policy_loss / self.epochs\n",
    "        avg_value_loss = total_value_loss / self.epochs\n",
    "        avg_entropy = total_entropy / self.epochs\n",
    "        avg_total_loss = total_loss / self.epochs\n",
    "        avg_clip_fraction = clip_fraction / self.epochs\n",
    "        avg_approx_kl = approx_kl_div / self.epochs\n",
    "\n",
    "        # Log update summary in more concise key=value format at INFO level\n",
    "        self.logger.info(\n",
    "            \"update_complete loss=%.4f policy_loss=%.4f value_loss=%.4f entropy=%.4f clip_frac=%.3f kl=%.5f explained_var=%.3f\",\n",
    "            avg_total_loss,\n",
    "            avg_policy_loss,\n",
    "            avg_value_loss,\n",
    "            avg_entropy,\n",
    "            avg_clip_fraction,\n",
    "            avg_approx_kl,\n",
    "            explained_var,\n",
    "        )\n",
    "\n",
    "        # Clear memory\n",
    "        self.memory.clear()\n",
    "\n",
    "        # Return metrics for potential further use\n",
    "        return {\n",
    "            \"loss\": avg_total_loss,\n",
    "            \"policy_loss\": avg_policy_loss,\n",
    "            \"value_loss\": avg_value_loss,\n",
    "            \"entropy\": avg_entropy,\n",
    "            \"clip_fraction\": avg_clip_fraction,\n",
    "            \"approx_kl\": avg_approx_kl,\n",
    "            \"explained_variance\": explained_var,\n",
    "        }\n",
    "\n",
    "    def save(self, path):\n",
    "        artifacts_dir = ensure_artifacts_dir()\n",
    "        full_path = os.path.join(artifacts_dir, path)\n",
    "\n",
    "        # Save both model and optimizer state for full resumability\n",
    "        checkpoint = {\n",
    "            \"model\": self.actor_critic.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"normalizer_stats\": None,  # If you ever add normalizer back\n",
    "            \"config\": {\n",
    "                \"state_dim\": self.actor_critic.shared[0].in_features,\n",
    "                \"action_dim\": len(self.actor_critic.log_std),\n",
    "                \"hidden_dim\": self.actor_critic.shared[0].out_features,\n",
    "                \"lr\": self.optimizer.param_groups[0][\"lr\"],\n",
    "                \"gamma\": self.gamma,\n",
    "                \"lam\": self.lam,\n",
    "                \"eps_clip\": self.eps_clip,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, full_path)\n",
    "        self.logger.info(\"model_saved path=%s\", full_path)\n",
    "\n",
    "    def load(self, path, load_optimizer=True):\n",
    "        if os.path.dirname(path) == \"\":\n",
    "            artifacts_dir = ensure_artifacts_dir()\n",
    "            path = os.path.join(artifacts_dir, path)\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "        self.actor_critic.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "        if load_optimizer and \"optimizer\" in checkpoint:\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.logger.info(\"model_loaded path=%s\", path)\n",
    "        return checkpoint.get(\"config\", {})  # Return config for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8492c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(\n",
    "    env,\n",
    "    agent,\n",
    "    max_episodes=500,\n",
    "    target_reward=0.0,\n",
    "    log_interval=20,\n",
    "    eval_interval=50,\n",
    "    steps_per_update=STEPS_PER_UPDATE,\n",
    "    logger=None,\n",
    "    experiment_name=\"\",\n",
    "):\n",
    "    # Initialize logger if not provided\n",
    "    if logger is None:\n",
    "        logger = setup_logger()\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    logger.info(f\"Device: {device_name}\")\n",
    "    logger.info(f\"Max episodes: {max_episodes}, Target reward: {target_reward}\")\n",
    "    logger.info(f\"Environment: {env.spec.id}\")\n",
    "    logger.info(f\"Steps per update: {steps_per_update}, PPO epochs: {agent.epochs}\")\n",
    "    logger.info(\n",
    "        f\"Learning rate: {agent.optimizer.param_groups[0]['lr']}, Gamma: {agent.gamma}, Lambda: {agent.lam}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Clip epsilon: {agent.eps_clip}, Value coef: {agent.value_coef}, Entropy coef: {agent.entropy_coef}\"\n",
    "    )\n",
    "\n",
    "    # For tracking progress\n",
    "    rewards = []  # Evaluation rewards\n",
    "    episode_rewards = []  # Individual episode rewards during training\n",
    "    avg_rewards = []  # Moving average of evaluation rewards\n",
    "    training_episodes = []  # To track episode numbers for plotting\n",
    "    eval_episodes = [0]  # To track episode numbers for evaluations\n",
    "    best_avg_reward = -float(\"inf\")\n",
    "\n",
    "    # For storing metrics\n",
    "    metrics_history = {\n",
    "        \"episode_rewards\": [],\n",
    "        \"eval_rewards\": [],\n",
    "        \"avg_eval_rewards\": [],\n",
    "        \"policy_updates\": [],\n",
    "        \"episode_numbers\": [],\n",
    "        \"eval_episode_numbers\": [],\n",
    "        \"timestamps\": [],\n",
    "    }\n",
    "\n",
    "    # For early stopping\n",
    "    solved = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_steps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    # Ensure artifacts directory exists\n",
    "    artifacts_dir = ensure_artifacts_dir()\n",
    "\n",
    "    # Do initial evaluation\n",
    "    logger.info(\"Performing initial evaluation...\")\n",
    "    eval_reward = evaluate(env, agent, num_episodes=5)\n",
    "    rewards.append(eval_reward)\n",
    "    avg_rewards.append(eval_reward)\n",
    "    metrics_history[\"eval_rewards\"].append(eval_reward)\n",
    "    metrics_history[\"avg_eval_rewards\"].append(eval_reward)\n",
    "    metrics_history[\"eval_episode_numbers\"].append(0)\n",
    "    metrics_history[\"timestamps\"].append(0)\n",
    "    logger.info(f\"initial_eval reward={eval_reward:.2f}\")\n",
    "\n",
    "    while episode_num < max_episodes:\n",
    "        # Collect a batch of transitions\n",
    "        steps_collected = 0\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        while steps_collected < steps_per_update and episode_num < max_episodes:\n",
    "            episode_num += 1\n",
    "            state, _ = env.reset(seed=SEED + episode_num)\n",
    "\n",
    "            # Flatten observation from (N, F) to (N*F,)\n",
    "            state = state.reshape(-1)\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            episode_steps = 0\n",
    "\n",
    "            while not done and steps_collected < steps_per_update:\n",
    "                # Select action with normalized state\n",
    "                action, pre_tanh_action, log_prob, value = agent.select_action(state)\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Flatten next_state from (N, F) to (N*F,)\n",
    "                next_state = next_state.reshape(-1)\n",
    "\n",
    "                # Store in memory (using normalized state and both action forms)\n",
    "                agent.memory.store(\n",
    "                    state,\n",
    "                    action,\n",
    "                    pre_tanh_action,\n",
    "                    reward,\n",
    "                    next_state,\n",
    "                    log_prob,\n",
    "                    done,\n",
    "                    value,\n",
    "                )\n",
    "\n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps_collected += 1\n",
    "                total_steps += 1\n",
    "                episode_steps += 1\n",
    "\n",
    "                # If we've collected enough steps or episode is done, we can stop\n",
    "                if steps_collected >= steps_per_update:\n",
    "                    break\n",
    "\n",
    "            # Record the completed episode's reward and episode number\n",
    "            episode_rewards.append(episode_reward)\n",
    "            training_episodes.append(episode_num)\n",
    "            metrics_history[\"episode_rewards\"].append(episode_reward)\n",
    "            metrics_history[\"episode_numbers\"].append(episode_num)\n",
    "\n",
    "            # Log episode info based on log_interval\n",
    "            if episode_num % log_interval == 0:\n",
    "                avg_ep_reward = np.mean(episode_rewards[-log_interval:])\n",
    "                elapsed_time = time.time() - start_time\n",
    "                logger.info(\n",
    "                    \"episode=%d reward=%.2f avg_reward=%.2f steps=%d episode_steps=%d time=%.2fs\",\n",
    "                    episode_num,\n",
    "                    episode_reward,\n",
    "                    avg_ep_reward,\n",
    "                    total_steps,\n",
    "                    episode_steps,\n",
    "                    elapsed_time,\n",
    "                )\n",
    "\n",
    "            # Check for evaluation based on eval_interval - do it here to ensure evaluations happen exactly every eval_interval episodes\n",
    "            if episode_num % eval_interval == 0:\n",
    "                logger.info(f\"Evaluating at episode {episode_num}...\")\n",
    "                eval_reward = evaluate(env, agent, num_episodes=5)\n",
    "                rewards.append(eval_reward)\n",
    "                eval_episodes.append(episode_num)\n",
    "                eval_time = time.time() - start_time\n",
    "\n",
    "                # Calculate average reward from last 10 evaluations\n",
    "                avg_reward = (\n",
    "                    np.mean(rewards[-10:]) if len(rewards) >= 10 else np.mean(rewards)\n",
    "                )\n",
    "                avg_rewards.append(avg_reward)\n",
    "\n",
    "                # Store in metrics\n",
    "                metrics_history[\"eval_rewards\"].append(eval_reward)\n",
    "                metrics_history[\"avg_eval_rewards\"].append(avg_reward)\n",
    "                metrics_history[\"eval_episode_numbers\"].append(episode_num)\n",
    "                metrics_history[\"timestamps\"].append(eval_time)\n",
    "\n",
    "                logger.info(\n",
    "                    \"eval episode=%d reward=%.2f avg_reward=%.2f time=%.2fs\",\n",
    "                    episode_num,\n",
    "                    eval_reward,\n",
    "                    avg_reward,\n",
    "                    eval_time,\n",
    "                )\n",
    "\n",
    "                # Check if environment is solved\n",
    "                if avg_reward >= target_reward and not solved and len(rewards) >= 10:\n",
    "                    logger.info(\n",
    "                        f\"Environment solved in {episode_num} episodes! Average reward: {avg_reward:.2f}\"\n",
    "                    )\n",
    "                    # Save the model\n",
    "                    agent.save(\"ppo_highway_solved.pth\")\n",
    "                    solved = True\n",
    "\n",
    "                # Save the best model\n",
    "                if avg_reward > best_avg_reward:\n",
    "                    best_avg_reward = avg_reward\n",
    "                    agent.save(\"ppo_highway_best.pth\")\n",
    "                    logger.info(\n",
    "                        f\"New best model saved with average reward: {best_avg_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "            # Only break outer loop if we've collected enough steps\n",
    "            if steps_collected >= steps_per_update:\n",
    "                break\n",
    "\n",
    "        # Calculate the final state value for bootstrapping\n",
    "        final_value = 0.0\n",
    "        if not done:  # If we stopped collection mid-episode\n",
    "            with torch.no_grad():\n",
    "                # Fix: Properly unpack three values (mean, std, value) from forward method\n",
    "                _, _, final_value = agent.actor_critic(state)\n",
    "                final_value = final_value.cpu().item()\n",
    "\n",
    "        # Update policy with proper bootstrapping after collecting full batch\n",
    "        logger.debug(f\"Updating policy after collecting {steps_collected} steps...\")\n",
    "        update_metrics = agent.update(last_value=final_value)\n",
    "        update_time = time.time() - update_start_time\n",
    "\n",
    "        # Store policy update metrics\n",
    "        metrics_history[\"policy_updates\"].append(\n",
    "            {\n",
    "                \"episode\": episode_num,\n",
    "                \"steps_collected\": steps_collected,\n",
    "                \"time\": update_time,\n",
    "                **update_metrics,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"Policy update completed in {update_time:.2f}s\")\n",
    "\n",
    "    # Save training metrics to JSON file\n",
    "    metrics_path = os.path.join(artifacts_dir, \"training_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics_history, f, indent=2)\n",
    "    logger.info(f\"Training metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa025294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(env, agent, num_episodes=10, render=False):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=SEED + 1000 + episode)\n",
    "\n",
    "        # Flatten observation\n",
    "        state = state.reshape(-1)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select action (deterministic) with normalized state\n",
    "            action, _, _, _ = agent.select_action(state, deterministic=True)\n",
    "\n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Flatten next_state\n",
    "            next_state = next_state.reshape(-1)\n",
    "\n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1605dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a trained agent\n",
    "def visualize_agent(env, agent, num_episodes=3, logger=None):\n",
    "    if logger is None:\n",
    "        logger = logging.getLogger(\"ppo_highway\")\n",
    "\n",
    "    logger.info(\"Visualizing agent...\")\n",
    "\n",
    "    # Register highway environment\n",
    "    if \"highway-v0\" not in gym.envs.registry:\n",
    "        highway_env._register_highway_envs()\n",
    "    logger.info(\"Registered highway-env in visualization process\")\n",
    "\n",
    "    # Create visualization environment once with render mode\n",
    "    env_viz = gym.make(\"highway-v0\", render_mode=\"human\", config=HIGHWAY_CONFIG)\n",
    "\n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env_viz.reset(seed=SEED + 2000 + episode)\n",
    "\n",
    "            # Flatten observation\n",
    "            state = state.reshape(-1)\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Select action (deterministic)\n",
    "                action, _, _, _ = agent.select_action(state, deterministic=True)\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env_viz.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Flatten next_state\n",
    "                next_state = next_state.reshape(-1)\n",
    "\n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            logger.info(\"viz_episode=%d reward=%.2f\", episode + 1, episode_reward)\n",
    "\n",
    "    finally:\n",
    "        # Ensure environment is closed even if an exception occurs\n",
    "        env_viz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f838c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create and wrap the environment\n",
    "    base_env = gym.make(\"highway-v0\", config=HIGHWAY_CONFIG)\n",
    "    env = HighwayEnvWrapper(base_env)\n",
    "\n",
    "    # Debugging shape info\n",
    "    print(\"Raw obs shape:\", base_env.observation_space.shape)\n",
    "    print(\"Wrapped obs shape:\", env.observation_space.shape)\n",
    "\n",
    "    # Determine dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Setup master logger\n",
    "    master_logger = setup_master_logger()\n",
    "    master_logger.info(f\"Starting Highway-Env PPO Training\")\n",
    "    master_logger.info(f\"Using {device_name} for training\")\n",
    "    master_logger.info(f\"State dimension: {state_dim}\")\n",
    "    master_logger.info(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "    # Flag to run a single training session instead of hyperparameter sweep\n",
    "    run_single = False\n",
    "\n",
    "    if run_single:\n",
    "        master_logger.info(\"\\n=== Running Single Training Session ===\")\n",
    "\n",
    "        # Create experiment logger\n",
    "        experiment_logger = setup_experiment_logger(\"single_run\", console_level=logging.INFO)\n",
    "\n",
    "        # Create PPO agent\n",
    "        agent = PPOAgent(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            lr=LEARNING_RATE,\n",
    "            gamma=GAMMA,\n",
    "            lam=LAMBDA,\n",
    "            eps_clip=EPSILON_CLIP,\n",
    "            value_coef=VALUE_COEF,\n",
    "            entropy_coef=ENTROPY_COEF,\n",
    "            max_grad_norm=MAX_GRAD_NORM,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            logger=experiment_logger,\n",
    "        )\n",
    "\n",
    "        # Train the agent\n",
    "        rewards, avg_rewards, metrics_history = train(\n",
    "            env=env,\n",
    "            agent=agent,\n",
    "            max_episodes=300,\n",
    "            target_reward=TARGET_REWARD,\n",
    "            log_interval=LOG_INTERVAL,\n",
    "            eval_interval=EVAL_INTERVAL,\n",
    "            steps_per_update=STEPS_PER_UPDATE,\n",
    "            logger=experiment_logger,\n",
    "            experiment_name=\"single_run\",\n",
    "        )\n",
    "\n",
    "        master_logger.info(f\"Single run completed with final reward: {avg_rewards[-1]:.2f}\")\n",
    "\n",
    "        # Visualize trained agent\n",
    "        experiment_logger.info(\"Visualizing trained agent...\")\n",
    "        if \"highway-v0\" not in gym.envs.registry:\n",
    "            highway_env._register_highway_envs()\n",
    "        viz_env = gym.make(\"highway-v0\", config=HIGHWAY_CONFIG, render_mode=\"human\")\n",
    "        visualize_agent(viz_env, agent, num_episodes=3, logger=experiment_logger)\n",
    "        viz_env.close()\n",
    "        env.close()\n",
    "\n",
    "    else:\n",
    "        # Run hyperparameter experiments\n",
    "        master_logger.info(\"Running hyperparameter experiments in parallel\")\n",
    "        run_experiments(\n",
    "            env=env,\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hyperparams_to_vary={\n",
    "                \"epochs\": [5, 10, 12],\n",
    "                \"lr\": [1e-4, 3e-4, 5e-5],\n",
    "                \"hidden_dim\": [64, 128, 256],\n",
    "                \"features\": [\n",
    "                    [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "                    [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "                    [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "                ],\n",
    "                \"batch_size\": [16, 32, 64, 128],\n",
    "            },\n",
    "            n_jobs=9,  # Adjust based on CPU\n",
    "            logger=master_logger,\n",
    "        )\n",
    "\n",
    "    master_logger.info(\"All experiments completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab2c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments with different hyperparameters\n",
    "def run_experiments(\n",
    "    env, state_dim, action_dim, hyperparams_to_vary, n_jobs=-1, logger=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run multiple experiments with different hyperparameter values in parallel.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to train on (will be closed and recreated in each worker)\n",
    "        state_dim: State dimension\n",
    "        action_dim: Action dimension\n",
    "        hyperparams_to_vary: Dict mapping hyperparameter names to lists of values to try\n",
    "        n_jobs: Number of parallel jobs to run (-1 for all available cores)\n",
    "        logger: Master logger instance for logging (should be created in the main process)\n",
    "    \"\"\"\n",
    "    # Initialize master logger if not provided\n",
    "    master_logger = logger or setup_master_logger()\n",
    "\n",
    "    # Default hyperparameters\n",
    "    default_hyperparams = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"gamma\": GAMMA,\n",
    "        \"lam\": LAMBDA,\n",
    "        \"eps_clip\": EPSILON_CLIP,\n",
    "        \"value_coef\": VALUE_COEF,\n",
    "        \"entropy_coef\": ENTROPY_COEF,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "    }\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    import itertools\n",
    "\n",
    "    param_names = list(hyperparams_to_vary.keys())\n",
    "    param_values = list(\n",
    "        itertools.product(*(hyperparams_to_vary[name] for name in param_names))\n",
    "    )\n",
    "\n",
    "    total_experiments = len(param_values)\n",
    "    master_logger.info(\n",
    "        f\"\\nRunning {total_experiments} experiments with varying {', '.join(param_names)} using {n_jobs} parallel workers.\\n\"\n",
    "    )\n",
    "\n",
    "    # We need to close the environment as each worker will create its own\n",
    "    env.close()\n",
    "\n",
    "    # Define the function to run a single experiment (to be executed in parallel)\n",
    "    def run_single_experiment(experiment_idx, values):\n",
    "        # Create a deep copy of the highway config to avoid cross-contamination\n",
    "        local_config = copy.deepcopy(HIGHWAY_CONFIG)\n",
    "\n",
    "        param_desc = []\n",
    "        feature_set = None\n",
    "\n",
    "        # Create unique experiment ID\n",
    "        experiment_id = f\"exp_{experiment_idx}\"\n",
    "\n",
    "        # Create experiment logger - only warnings and errors go to console\n",
    "        experiment_logger = setup_experiment_logger(experiment_id)\n",
    "\n",
    "        # First, extract any special hyperparameters like \"features\"\n",
    "        for name, value in zip(param_names, values):\n",
    "            if name == \"features\":\n",
    "                # Store the feature set for environment creation\n",
    "                feature_set = value\n",
    "                # Create a shorter representation for the experiment name\n",
    "                feature_str = \",\".join(value)\n",
    "                param_desc.append(f\"feat={feature_str}\")\n",
    "\n",
    "        # Update the config with the feature set if provided\n",
    "        if feature_set is not None:\n",
    "            local_config[\"observation\"][\"features\"] = feature_set\n",
    "\n",
    "        # Register highway environment\n",
    "        if \"highway-v0\" not in gym.envs.registry:\n",
    "            highway_env._register_highway_envs()\n",
    "            experiment_logger.info(\"Registered highway-env in worker process\")\n",
    "\n",
    "        # Create a new environment for this worker with the potentially modified config\n",
    "        worker_env = gym.make(\"highway-v0\", config=local_config)\n",
    "\n",
    "        # Calculate state dimension by flattening observation space\n",
    "        worker_state_dim = np.prod(worker_env.observation_space.shape)\n",
    "        worker_action_dim = worker_env.action_space.shape[0]\n",
    "\n",
    "        # Set a unique seed for this experiment\n",
    "        worker_seed = SEED + experiment_idx * 1000\n",
    "        random.seed(worker_seed)\n",
    "        np.random.seed(worker_seed)\n",
    "        torch.manual_seed(worker_seed)\n",
    "\n",
    "        # Create hyperparameter set for this experiment\n",
    "        experiment_hyperparams = default_hyperparams.copy()\n",
    "        # Update state_dim to worker's calculated dimension\n",
    "        experiment_hyperparams[\"state_dim\"] = worker_state_dim\n",
    "        experiment_hyperparams[\"action_dim\"] = worker_action_dim\n",
    "\n",
    "        # Now add the remaining scalar hyperparameters\n",
    "        for name, value in zip(param_names, values):\n",
    "            if name != \"features\":  # Skip features as they're handled differently\n",
    "                experiment_hyperparams[name] = value\n",
    "                param_desc.append(f\"{name}={value}\")\n",
    "\n",
    "        experiment_name = \"_\".join(param_desc)\n",
    "        experiment_logger.info(\n",
    "            f\"\\n=== Starting Experiment {experiment_idx + 1}/{total_experiments}: {experiment_name} ===\\n\"\n",
    "        )\n",
    "        experiment_logger.info(\n",
    "            f\"State dimension: {worker_state_dim}, Action dimension: {worker_action_dim}\"\n",
    "        )\n",
    "\n",
    "        if feature_set is not None:\n",
    "            experiment_logger.info(f\"Using features: {feature_set}\")\n",
    "\n",
    "        # Create agent with these hyperparameters\n",
    "        agent = PPOAgent(**experiment_hyperparams, logger=experiment_logger)\n",
    "\n",
    "        # Train agent with modified model saving to include hyperparameter values\n",
    "        rewards, avg_rewards, metrics_history = train_with_experiment_name(\n",
    "            env=worker_env,\n",
    "            agent=agent,\n",
    "            max_episodes=MAX_EPISODES,\n",
    "            target_reward=TARGET_REWARD,\n",
    "            log_interval=LOG_INTERVAL,\n",
    "            eval_interval=EVAL_INTERVAL,\n",
    "            steps_per_update=STEPS_PER_UPDATE,\n",
    "            experiment_name=experiment_name,\n",
    "            logger=experiment_logger,\n",
    "        )\n",
    "\n",
    "        # Log completion to experiment logger\n",
    "        experiment_logger.info(\n",
    "            f\"Experiment {experiment_idx + 1}/{total_experiments} completed. \"\n",
    "            f\"Final avg reward: {avg_rewards[-1]:.2f}, Max avg reward: {max(avg_rewards):.2f}\"\n",
    "        )\n",
    "\n",
    "        # Log a warning-level message that will appear in the console\n",
    "        experiment_logger.warning(\n",
    "            f\"Experiment {experiment_name} completed! Final reward: {avg_rewards[-1]:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Close the worker environment\n",
    "        worker_env.close()\n",
    "\n",
    "        # Return the results for this experiment\n",
    "        return {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"hyperparams\": experiment_hyperparams.copy(),\n",
    "            \"features\": feature_set,  # Store the feature set used\n",
    "            \"final_avg_reward\": avg_rewards[-1],\n",
    "            \"max_avg_reward\": max(avg_rewards),\n",
    "            \"rewards\": rewards,\n",
    "            \"avg_rewards\": avg_rewards,\n",
    "            \"metrics_history\": metrics_history,\n",
    "        }\n",
    "\n",
    "    # Run experiments in parallel\n",
    "    master_logger.info(\"Starting parallel experiments...\")\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "        delayed(run_single_experiment)(i, values)\n",
    "        for i, values in enumerate(param_values)\n",
    "    )\n",
    "\n",
    "    # Convert results list to dictionary\n",
    "    experiment_results = {result[\"experiment_name\"]: result for result in results}\n",
    "\n",
    "    # Summarize results in master logger\n",
    "    master_logger.info(\"\\n=== Experiment Results Summary ===\")\n",
    "    master_logger.info(\"experiment_name final_reward max_reward\")\n",
    "    master_logger.info(\"-\" * 70)\n",
    "\n",
    "    for exp_name, results in experiment_results.items():\n",
    "        master_logger.info(\n",
    "            \"exp=%s final_reward=%.2f max_reward=%.2f\",\n",
    "            exp_name,\n",
    "            results[\"final_avg_reward\"],\n",
    "            results[\"max_avg_reward\"],\n",
    "        )\n",
    "\n",
    "    # Visualize the best models after all experiments are done (optional)\n",
    "    visualize_best_models = True\n",
    "    if visualize_best_models:\n",
    "        master_logger.info(\"\\n=== Visualizing Best Models ===\")\n",
    "\n",
    "        for exp_name, result in experiment_results.items():\n",
    "            master_logger.info(f\"\\nVisualizing agent for experiment: {exp_name}\")\n",
    "\n",
    "            # Create a visualization environment with the correct feature set\n",
    "            viz_config = copy.deepcopy(HIGHWAY_CONFIG)\n",
    "            if result.get(\"features\") is not None:\n",
    "                viz_config[\"observation\"][\"features\"] = result[\"features\"]\n",
    "                master_logger.info(f\"Using features: {result['features']}\")\n",
    "\n",
    "            # Register highway environment\n",
    "            if \"highway-v0\" not in gym.envs.registry:\n",
    "                highway_env._register_highway_envs()\n",
    "\n",
    "            viz_env = gym.make(\"highway-v0\", config=viz_config, render_mode=\"human\")\n",
    "\n",
    "            artifacts_dir = ensure_artifacts_dir()\n",
    "            best_model_path = os.path.join(\n",
    "                artifacts_dir, f\"ppo_highway_best_{exp_name}.pth\"\n",
    "            )\n",
    "\n",
    "            # Create a new agent and load the model\n",
    "            agent = PPOAgent(\n",
    "                state_dim=result[\"hyperparams\"][\"state_dim\"],\n",
    "                action_dim=result[\"hyperparams\"][\"action_dim\"],\n",
    "                **{\n",
    "                    k: v\n",
    "                    for k, v in result[\"hyperparams\"].items()\n",
    "                    if k not in [\"state_dim\", \"action_dim\"]\n",
    "                },\n",
    "                logger=master_logger,\n",
    "            )\n",
    "            agent.load(best_model_path)\n",
    "\n",
    "            # Visualize\n",
    "            visualize_agent(viz_env, agent, num_episodes=1, logger=master_logger)\n",
    "\n",
    "            # Close visualization environment after each experiment\n",
    "            viz_env.close()\n",
    "\n",
    "    # Plot comparison of learning curves\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    for exp_name, results in experiment_results.items():\n",
    "        eval_episodes = list(\n",
    "            range(0, len(results[\"avg_rewards\"]) * EVAL_INTERVAL, EVAL_INTERVAL)\n",
    "        )\n",
    "        if len(eval_episodes) != len(results[\"avg_rewards\"]):\n",
    "            eval_episodes = [0] + eval_episodes\n",
    "        plt.plot(eval_episodes, results[\"avg_rewards\"], \"-o\", label=exp_name)\n",
    "\n",
    "    plt.axhline(y=TARGET_REWARD, color=\"r\", linestyle=\"--\", label=\"Target Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Comparison of Hyperparameter Settings\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save comparison plot\n",
    "    artifacts_dir = ensure_artifacts_dir()\n",
    "    comparison_plot_path = os.path.join(artifacts_dir, \"hyperparameter_comparison.png\")\n",
    "    plt.savefig(comparison_plot_path)\n",
    "    plt.close()\n",
    "    master_logger.info(f\"\\nComparison plot saved to {comparison_plot_path}\")\n",
    "\n",
    "    # Save combined metrics to JSON\n",
    "    metrics_path = os.path.join(artifacts_dir, \"all_experiments_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"experiments\": {\n",
    "                    exp_name: {\n",
    "                        \"hyperparams\": results[\"hyperparams\"],\n",
    "                        \"features\": results[\"features\"],\n",
    "                        \"final_avg_reward\": results[\"final_avg_reward\"],\n",
    "                        \"max_avg_reward\": results[\"max_avg_reward\"],\n",
    "                    }\n",
    "                    for exp_name, results in experiment_results.items()\n",
    "                }\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    master_logger.info(f\"Combined experiment metrics saved to {metrics_path}\")\n",
    "\n",
    "    # Create a master CSV with results from all experiments\n",
    "    master_csv_path = os.path.join(artifacts_dir, \"all_experiments_summary.csv\")\n",
    "    with open(master_csv_path, \"w\") as f:\n",
    "        # Write header with all possible hyperparameters\n",
    "        f.write(\"experiment_name,final_reward,max_reward,\")\n",
    "        f.write(\",\".join(default_hyperparams.keys()))\n",
    "        f.write(\",plot_path,best_model_path\\n\")\n",
    "\n",
    "        # Write one row per experiment\n",
    "        for exp_name, results in experiment_results.items():\n",
    "            f.write(\n",
    "                f\"{exp_name},{results['final_avg_reward']:.4f},{results['max_avg_reward']:.4f},\"\n",
    "            )\n",
    "            # Write hyperparameter values\n",
    "            for param, default in default_hyperparams.items():\n",
    "                if param in results[\"hyperparams\"]:\n",
    "                    f.write(f\"{results['hyperparams'][param]},\")\n",
    "                else:\n",
    "                    f.write(f\"{default},\")\n",
    "            # Write paths\n",
    "            f.write(\n",
    "                f\"{os.path.join(artifacts_dir, f'ppo_highway_rewards_{exp_name}.png')},\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"{os.path.join(artifacts_dir, f'ppo_highway_best_{exp_name}.pth')}\\n\"\n",
    "            )\n",
    "\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24bb33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified training function for experiments\n",
    "def train_with_experiment_name(\n",
    "    env,\n",
    "    agent,\n",
    "    max_episodes=500,\n",
    "    target_reward=0.0,\n",
    "    log_interval=20,\n",
    "    eval_interval=50,\n",
    "    steps_per_update=STEPS_PER_UPDATE,\n",
    "    experiment_name=\"\",\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"Modified version of train() that includes experiment_name in saved artifacts\"\"\"\n",
    "    # Initialize logger with experiment name if not provided\n",
    "    if logger is None:\n",
    "        logger = setup_experiment_logger(experiment_name)\n",
    "\n",
    "    # Use a consistent prefix for experiment logs\n",
    "    exp_prefix = f\"[{experiment_name}]\" if experiment_name else \"\"\n",
    "\n",
    "    logger.info(f\"{exp_prefix} Starting training for experiment: {experiment_name}\")\n",
    "    logger.info(f\"{exp_prefix} Device: {device_name}\")\n",
    "    logger.info(\n",
    "        f\"{exp_prefix} Max episodes: {max_episodes}, Target reward: {target_reward}\"\n",
    "    )\n",
    "    logger.info(f\"{exp_prefix} Environment: {env.spec.id}\")\n",
    "    logger.info(\n",
    "        f\"{exp_prefix} Steps per update: {steps_per_update}, PPO epochs: {agent.epochs}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{exp_prefix} Learning rate: {agent.optimizer.param_groups[0]['lr']}, Gamma: {agent.gamma}, Lambda: {agent.lam}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{exp_prefix} Clip epsilon: {agent.eps_clip}, Value coef: {agent.value_coef}, Entropy coef: {agent.entropy_coef}\"\n",
    "    )\n",
    "\n",
    "    # For tracking progress\n",
    "    rewards = []  # Evaluation rewards\n",
    "    episode_rewards = []  # Individual episode rewards during training\n",
    "    avg_rewards = []  # Moving average of evaluation rewards\n",
    "    training_episodes = []  # To track episode numbers for plotting\n",
    "    eval_episodes = [0]  # To track episode numbers for evaluations\n",
    "    best_avg_reward = -float(\"inf\")\n",
    "\n",
    "    # For storing metrics\n",
    "    metrics_history = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"episode_rewards\": [],\n",
    "        \"eval_rewards\": [],\n",
    "        \"avg_eval_rewards\": [],\n",
    "        \"policy_updates\": [],\n",
    "        \"episode_numbers\": [],\n",
    "        \"eval_episode_numbers\": [],\n",
    "        \"timestamps\": [],\n",
    "    }\n",
    "\n",
    "    # For early stopping\n",
    "    solved = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_steps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    # Ensure artifacts directory exists\n",
    "    artifacts_dir = ensure_artifacts_dir()\n",
    "\n",
    "    # Do initial evaluation\n",
    "    logger.info(f\"{exp_prefix} Performing initial evaluation...\")\n",
    "    eval_reward = evaluate(env, agent, num_episodes=5)\n",
    "    rewards.append(eval_reward)\n",
    "    avg_rewards.append(eval_reward)\n",
    "    metrics_history[\"eval_rewards\"].append(eval_reward)\n",
    "    metrics_history[\"avg_eval_rewards\"].append(eval_reward)\n",
    "    metrics_history[\"eval_episode_numbers\"].append(0)\n",
    "    metrics_history[\"timestamps\"].append(0)\n",
    "    logger.info(f\"{exp_prefix} initial_eval reward={eval_reward:.2f}\")\n",
    "\n",
    "    while episode_num < max_episodes:\n",
    "        # Collect a batch of transitions\n",
    "        steps_collected = 0\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        while steps_collected < steps_per_update and episode_num < max_episodes:\n",
    "            episode_num += 1\n",
    "            state, _ = env.reset(seed=SEED + episode_num)\n",
    "\n",
    "            # Flatten observation\n",
    "            state = state.reshape(-1)\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            episode_steps = 0\n",
    "\n",
    "            while not done and steps_collected < steps_per_update:\n",
    "                # Select action with state\n",
    "                action, pre_tanh_action, log_prob, value = agent.select_action(state)\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Flatten next_state\n",
    "                next_state = next_state.reshape(-1)\n",
    "\n",
    "                # Store in memory (both action forms)\n",
    "                agent.memory.store(\n",
    "                    state,\n",
    "                    action,\n",
    "                    pre_tanh_action,\n",
    "                    reward,\n",
    "                    next_state,\n",
    "                    log_prob,\n",
    "                    done,\n",
    "                    value,\n",
    "                )\n",
    "\n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps_collected += 1\n",
    "                total_steps += 1\n",
    "                episode_steps += 1\n",
    "\n",
    "                # If we've collected enough steps or episode is done, we can stop\n",
    "                if steps_collected >= steps_per_update:\n",
    "                    break\n",
    "\n",
    "            # Record the completed episode's reward and episode number\n",
    "            episode_rewards.append(episode_reward)\n",
    "            training_episodes.append(episode_num)\n",
    "            metrics_history[\"episode_rewards\"].append(episode_reward)\n",
    "            metrics_history[\"episode_numbers\"].append(episode_num)\n",
    "\n",
    "            # Log episode info based on log_interval\n",
    "            if episode_num % log_interval == 0:\n",
    "                avg_ep_reward = np.mean(episode_rewards[-log_interval:])\n",
    "                elapsed_time = time.time() - start_time\n",
    "                logger.info(\n",
    "                    \"%s episode=%d reward=%.2f avg_reward=%.2f steps=%d episode_steps=%d time=%.2fs\",\n",
    "                    exp_prefix,\n",
    "                    episode_num,\n",
    "                    episode_reward,\n",
    "                    avg_ep_reward,\n",
    "                    total_steps,\n",
    "                    episode_steps,\n",
    "                    elapsed_time,\n",
    "                )\n",
    "\n",
    "            # Check for evaluation based on eval_interval\n",
    "            if episode_num % eval_interval == 0:\n",
    "                logger.info(f\"{exp_prefix} Evaluating at episode {episode_num}...\")\n",
    "                eval_reward = evaluate(env, agent, num_episodes=5)\n",
    "                rewards.append(eval_reward)\n",
    "                eval_episodes.append(episode_num)\n",
    "                eval_time = time.time() - start_time\n",
    "\n",
    "                # Calculate average reward from last 10 evaluations\n",
    "                avg_reward = (\n",
    "                    np.mean(rewards[-10:]) if len(rewards) >= 10 else np.mean(rewards)\n",
    "                )\n",
    "                avg_rewards.append(avg_reward)\n",
    "\n",
    "                # Store in metrics\n",
    "                metrics_history[\"eval_rewards\"].append(eval_reward)\n",
    "                metrics_history[\"avg_eval_rewards\"].append(avg_reward)\n",
    "                metrics_history[\"eval_episode_numbers\"].append(episode_num)\n",
    "                metrics_history[\"timestamps\"].append(eval_time)\n",
    "\n",
    "                logger.info(\n",
    "                    \"%s eval episode=%d reward=%.2f avg_reward=%.2f time=%.2fs\",\n",
    "                    exp_prefix,\n",
    "                    episode_num,\n",
    "                    eval_reward,\n",
    "                    avg_reward,\n",
    "                    eval_time,\n",
    "                )\n",
    "\n",
    "                # Check if environment is solved\n",
    "                if avg_reward >= target_reward and not solved and len(rewards) >= 10:\n",
    "                    logger.info(\n",
    "                        f\"{exp_prefix} Environment solved in {episode_num} episodes! Average reward: {avg_reward:.2f}\"\n",
    "                    )\n",
    "                    # Save the model with experiment name\n",
    "                    agent.save(f\"ppo_highway_solved_{experiment_name}.pth\")\n",
    "                    solved = True\n",
    "\n",
    "                # Save the best model with experiment name\n",
    "                if avg_reward > best_avg_reward:\n",
    "                    best_avg_reward = avg_reward\n",
    "                    agent.save(f\"ppo_highway_best_{experiment_name}.pth\")\n",
    "                    logger.info(\n",
    "                        f\"{exp_prefix} New best model saved with average reward: {best_avg_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "            # Only break outer loop if we've collected enough steps\n",
    "            if steps_collected >= steps_per_update:\n",
    "                break\n",
    "\n",
    "        # Calculate the final state value for bootstrapping\n",
    "        final_value = 0.0\n",
    "        if not done:  # If we stopped collection mid-episode\n",
    "            with torch.no_grad():\n",
    "                # Get final value directly from state\n",
    "                _, _, final_value = agent.actor_critic(state)\n",
    "                final_value = final_value.cpu().item()\n",
    "\n",
    "        # Update policy with proper bootstrapping after collecting full batch\n",
    "        logger.debug(f\"Updating policy after collecting {steps_collected} steps...\")\n",
    "        update_metrics = agent.update(last_value=final_value)\n",
    "        update_time = time.time() - update_start_time\n",
    "\n",
    "        # Store policy update metrics\n",
    "        metrics_history[\"policy_updates\"].append(\n",
    "            {\n",
    "                \"episode\": episode_num,\n",
    "                \"steps_collected\": steps_collected,\n",
    "                \"time\": update_time,\n",
    "                **update_metrics,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"Policy update completed in {update_time:.2f}s\")\n",
    "\n",
    "    # Save training metrics to JSON file\n",
    "    metrics_path = os.path.join(\n",
    "        artifacts_dir, f\"training_metrics_{experiment_name}.json\"\n",
    "    )\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics_history, f, indent=2)\n",
    "    logger.info(f\"{exp_prefix} Training metrics saved to {metrics_path}\")\n",
    "\n",
    "    # Plot rewards\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training rewards\n",
    "    plt.plot(\n",
    "        training_episodes,\n",
    "        episode_rewards,\n",
    "        alpha=0.3,\n",
    "        color=\"gray\",\n",
    "        label=\"Training Episode Reward\",\n",
    "    )\n",
    "\n",
    "    # Plot smoothed training rewards\n",
    "    window_size = 20\n",
    "    if len(episode_rewards) > window_size:\n",
    "        smoothed_rewards = np.convolve(\n",
    "            episode_rewards, np.ones(window_size) / window_size, mode=\"valid\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            training_episodes[window_size - 1 :],\n",
    "            smoothed_rewards,\n",
    "            color=\"blue\",\n",
    "            label=f\"Training Reward (Moving Avg {window_size})\",\n",
    "        )\n",
    "\n",
    "    # Plot evaluation rewards\n",
    "    plt.plot(\n",
    "        eval_episodes,\n",
    "        rewards,\n",
    "        \"ro-\",\n",
    "        label=\"Evaluation Reward (5 episodes)\",\n",
    "        markersize=8,\n",
    "    )\n",
    "\n",
    "    # Plot average evaluation rewards\n",
    "    plt.plot(\n",
    "        eval_episodes,\n",
    "        avg_rewards,\n",
    "        \"go-\",\n",
    "        label=\"Evaluation Reward (Moving Avg)\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "    # Plot target line\n",
    "    plt.axhline(y=target_reward, color=\"r\", linestyle=\"--\", label=\"Target Reward\")\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"PPO Training Progress on Highway-v0 ({experiment_name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Save plot to artifacts directory with experiment name\n",
    "    plot_path = os.path.join(\n",
    "        artifacts_dir, f\"ppo_highway_rewards_{experiment_name}.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"{exp_prefix} Training plot saved to {plot_path}\")\n",
    "\n",
    "    # Create CSV summary for this run\n",
    "    csv_path = os.path.join(artifacts_dir, f\"summary_{experiment_name}.csv\")\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(\n",
    "            \"experiment,final_reward,max_reward,training_steps,best_model_path,plot_path\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"{experiment_name},{avg_rewards[-1]:.4f},{max(avg_rewards):.4f},{total_steps},\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"{os.path.join(artifacts_dir, f'ppo_highway_best_{experiment_name}.pth')},\"\n",
    "        )\n",
    "        f.write(f\"{plot_path}\\n\")\n",
    "\n",
    "    # Also update metrics_history to include artifact paths\n",
    "    metrics_history[\"best_model_path\"] = os.path.join(\n",
    "        artifacts_dir, f\"ppo_highway_best_{experiment_name}.pth\"\n",
    "    )\n",
    "    metrics_history[\"plot_path\"] = plot_path\n",
    "\n",
    "    logger.info(f\"{exp_prefix} Training completed!\")\n",
    "    return rewards, avg_rewards, metrics_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a2d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log_prob(pre_tanh_action, action):\n",
    "    # More robust version with clipping\n",
    "    correction = torch.log(1 - action.pow(2) + 1e-6)\n",
    "    # Add clipping to avoid extreme values\n",
    "    correction = torch.clamp(correction, min=-20.0)\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f326550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw obs shape: (15, 4)\n",
      "Wrapped obs shape: (60,)\n",
      "21:23:58 | INFO | Master logger initialized. Log file: artifacts\\highway-ppo\\logs\\20250419_212358_master.log\n",
      "21:23:58 | INFO | Starting Highway-Env PPO Training\n",
      "21:23:58 | INFO | Using CPU for training\n",
      "21:23:58 | INFO | State dimension: 60\n",
      "21:23:58 | INFO | Action dimension: 2\n",
      "21:23:58 | INFO | Running hyperparameter experiments in parallel\n",
      "21:23:58 | INFO | \n",
      "Running 324 experiments with varying epochs, lr, hidden_dim, features, batch_size using 9 parallel workers.\n",
      "\n",
      "21:23:58 | INFO | Starting parallel experiments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=9)]: Using backend LokyBackend with 9 concurrent workers.\n",
      "[Parallel(n_jobs=9)]: Done   7 tasks      | elapsed: 591.1min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 74\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Run hyperparameter experiments\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     master_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning hyperparameter experiments in parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparams_to_vary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcos_h\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msin_h\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on CPU\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaster_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m master_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll experiments completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 164\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m(env, state_dim, action_dim, hyperparams_to_vary, n_jobs, logger)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Run experiments in parallel\u001b[39;00m\n\u001b[0;32m    163\u001b[0m master_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting parallel experiments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_single_experiment\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparam_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Convert results list to dictionary\u001b[39;00m\n\u001b[0;32m    170\u001b[0m experiment_results \u001b[38;5;241m=\u001b[39m {result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]: result \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23d97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd013a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
